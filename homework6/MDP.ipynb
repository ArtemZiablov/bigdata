{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Додавання необхідних бібліотек"
      ],
      "metadata": {
        "id": "CfW0uWNtkgGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "otTDGiBhkXjL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Теоретичні Вказівки\n",
        "\n",
        "Процес Маркова прийняття рішень $(MDP)$ представляє систему, яка визначається станами $S$, діями $A$, ймовірностями переходу між станами $P_{s,a}$, фактором дисконтування, або discount factor -- $γ$, і функцією винагороди $R$.\n",
        "\n",
        "Ціль полягає у максимізації очікуваної винагороди, керуючи процесом через вибір оптимальної послідовності дій або політики $π$.\n",
        "\n",
        "Алгоритми для пошуку оптимальної політики включають ітерацію політики (Алгоритм 5) та навчання моделі для невідомих MDP (Алгоритм 6).\n"
      ],
      "metadata": {
        "id": "5BcKWRiOT9DS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Алгоритм 5: Ітерація політики\n",
        "\n",
        "Цей алгоритм використовується для знаходження оптимальної політики у процесі Маркова прийняття рішень $(MDP)$.\n",
        "Він поетапно оновлює політику та функцію вартості для кожного стану, поки не досягне оптимального результату або поки не завершаться ітерації.\n",
        "Алгоритм включає два основних кроки: оцінку політики та поліпшення політики.\n",
        "Оцінка політики обчислює значення функції вартості для поточної політики, а поліпшення оновлює політику для максимізації очікуваної винагороди.\n",
        "\n",
        "### Параметри:\n",
        "\n",
        "1. $states$: Список станів у системі (наприклад, [0, 1, 2, 3]).\n",
        "\n",
        "2. $actions$: Список можливих дій, які можна виконувати у кожному стані (наприклад, [0, 1]).\n",
        "\n",
        "3. $transition\\_prob$: Ймовірності переходу між станами для кожної дії (наприклад, вкладений словник, що містить ймовірності переходів між станами для кожної дії).\n",
        "\n",
        "4. $rewards$: Список винагород для кожного стану.\n",
        "\n",
        "5. $gamma$ (опціонально): Фактор дисконтування, який визначає значущість майбутніх винагород (значення за замовчуванням — 0.9).\n",
        "\n",
        "6. $max\\_iterations$ (опціонально): Максимальна кількість ітерацій для збіжності алгоритму (значення за замовчуванням — 1000).\n",
        "\n",
        "7. $tol$ (опціонально): Допуск для збіжності значень функції вартості (значення за замовчуванням — 1e-6)."
      ],
      "metadata": {
        "id": "XMF0KlQDWWyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ozi2Nwo8QOAw"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(states, actions, transition_prob, rewards, gamma=0.9, max_iterations=1000, tol=1e-6):\n",
        "    policy = np.random.choice(actions, size=len(states))\n",
        "    value_function = np.zeros(len(states))\n",
        "\n",
        "    is_converged = False\n",
        "    iteration = 0\n",
        "    while not is_converged and iteration < max_iterations:\n",
        "        # Крок a: Оцінка Політики\n",
        "        # На цьому кроці обчислюємо значення функції вартості для поточної політики, доки зміни значень не стануть менше tol.\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(len(states)):\n",
        "                v = value_function[s]\n",
        "                action = policy[s]\n",
        "                value_function[s] = rewards[s] + gamma * sum(\n",
        "                    transition_prob[s][action][s_next] * value_function[s_next] for s_next in range(len(states))\n",
        "                )\n",
        "                delta = max(delta, abs(v - value_function[s]))\n",
        "            if delta < tol:\n",
        "                break\n",
        "\n",
        "        # Крок b: Поліпшення Політики\n",
        "        # На цьому кроці для кожного стану обираємо дію, яка максимізує очікувану винагороду, використовуючи поточну функцію вартості.\n",
        "        policy_stable = True\n",
        "        for s in range(len(states)):\n",
        "            old_action = policy[s]\n",
        "            policy[s] = max(actions, key=lambda a: sum(\n",
        "                transition_prob[s][a][s_next] * value_function[s_next] for s_next in range(len(states))\n",
        "            ))\n",
        "            if old_action != policy[s]:\n",
        "                policy_stable = False\n",
        "\n",
        "        if policy_stable:\n",
        "            is_converged = True\n",
        "        iteration += 1\n",
        "\n",
        "    return policy, value_function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Алгоритм 6: Навчання моделі MDP з невідомими ймовірностями переходу\n",
        "\n",
        "Цей алгоритм використовується для навчання процесу Маркова прийняття рішень, коли ймовірності переходу між станами невідомі.\n",
        "Алгоритм включає збір даних про переходи та винагороди під час виконання поточної політики, оцінку ймовірностей переходу, та оновлення політики.\n",
        "Алгоритм працює у кілька ітерацій: виконується поточна політика, оновлюються оцінки ймовірностей та визначається нова оптимальна політика.\n",
        "\n",
        "### Параметри:\n",
        "\n",
        "1. $states$: Список станів у системі (наприклад, [0, 1, 2, 3]).\n",
        "\n",
        "2. $actions$: Список можливих дій, які можна виконувати у кожному стані (наприклад, [0, 1]).\n",
        "\n",
        "3. $transition\\_prob$: Ймовірності переходу між станами для кожної дії (наприклад, вкладений словник, що містить ймовірності переходів між станами для кожної дії).\n",
        "\n",
        "4. $rewards$: Список винагород для кожного стану.\n",
        "\n",
        "5. $gamma$ (опціонально): Фактор дисконтування, який визначає значущість майбутніх винагород (значення за замовчуванням — 0.9).\n",
        "\n",
        "6. $max\\_iterations$ (опціонально): Максимальна кількість ітерацій для збіжності алгоритму (значення за замовчуванням — 1000).\n",
        "\n",
        "7. $tol$ (опціонально): Допуск для збіжності значень функції вартості (значення за замовчуванням — 1e-6).\n",
        "\n"
      ],
      "metadata": {
        "id": "nfdtIes5Wp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_mdp_with_unknown_transitions(states, actions, transition_prob, rewards, gamma=0.9, max_iterations=1000, tol=1e-6):\n",
        "    policy = np.random.choice(actions, size=len(states))\n",
        "    transition_counts = {s: {a: np.zeros(len(states)) for a in actions} for s in states}\n",
        "\n",
        "    is_converged = False\n",
        "    iteration = 0\n",
        "    while not is_converged and iteration < max_iterations:\n",
        "        # Крок a: Виконання політики у MDP протягом деякої кількості спроб\n",
        "        # Виконуємо поточну політику, щоб зібрати дані про переходи між станами та винагороди\n",
        "        for _ in range(200):  # Збільшена кількість спроб для кращого збору даних\n",
        "            s = np.random.choice(states)\n",
        "            while True:\n",
        "                a = policy[s]\n",
        "                # Нормалізувати ймовірності переходів, щоб їх сума дорівнювала 1\n",
        "                if np.sum(transition_prob[s][a]) == 0:\n",
        "                    probs = np.ones(len(states)) / len(states)  # Якщо сума ймовірностей дорівнює 0, використовуємо рівномірний розподіл\n",
        "                else:\n",
        "                    probs = transition_prob[s][a] / np.sum(transition_prob[s][a])\n",
        "                s_next = np.random.choice(states, p=probs)  # Симуляція переходу на основі ймовірностей\n",
        "                reward = rewards[s]  # Симуляція винагороди (може бути замінена реальними даними на практиці)\n",
        "                transition_counts[s][a][s_next] += 1\n",
        "                s = s_next\n",
        "                if np.random.rand() < 0.2:  # Збільшена ймовірність завершення спроби для більшого варіювання\n",
        "                    break\n",
        "\n",
        "        # Крок b: Оновлення оцінок ймовірностей переходів\n",
        "        # Обчислюємо ймовірності переходів на основі зібраних даних (частота переходів)\n",
        "        transition_prob = {\n",
        "            s: {\n",
        "                a: transition_counts[s][a] / (np.sum(transition_counts[s][a]) + 1e-6)\n",
        "                for a in actions\n",
        "            }\n",
        "            for s in states\n",
        "        }\n",
        "\n",
        "        # Крок c: Оновлення політики за допомогою Алгоритму 5\n",
        "        # Використовуємо ітерацію політики, щоб оновити політику на основі нових оцінених ймовірностей переходу\n",
        "        policy, _ = policy_iteration(states, actions, transition_prob, rewards, gamma, max_iterations, tol)\n",
        "        iteration += 1\n",
        "\n",
        "    return policy\n"
      ],
      "metadata": {
        "id": "G9MOEzmuin8T"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Приклад використання"
      ],
      "metadata": {
        "id": "QNsBuGLcipOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = [0, 1, 2, 3]\n",
        "actions = [0, 1]  # Приклад дій (наприклад, 0: залишитися, 1: рухатися)\n",
        "transition_prob = {\n",
        "    0: {0: [0.3, 0.7, 0, 0], 1: [0.2, 0.3, 0.5, 0]},\n",
        "    1: {0: [0.4, 0.6, 0, 0], 1: [0.1, 0.6, 0.3, 0]},\n",
        "    2: {0: [0.1, 0.5, 0.4, 0], 1: [0.2, 0.4, 0.4, 0]},\n",
        "    3: {0: [0.3, 0.3, 0.4, 0], 1: [0.4, 0.4, 0, 0.2]},\n",
        "}\n",
        "rewards = [2, -1, 5, 3]\n",
        "\n",
        "# Запуск ітерації політики з різними значеннями gamma для оцінки впливу на результати\n",
        "gamma_values = [0.5, 0.9, 0.99]\n",
        "for gamma in gamma_values:\n",
        "    optimal_policy, optimal_value_function = policy_iteration(states, actions, transition_prob, rewards, gamma=gamma)\n",
        "    print(f\"\\nGamma: {gamma}\")\n",
        "    print(\"Optimal Policy:\", optimal_policy)\n",
        "    print(\"Optimal Value Function:\", optimal_value_function)\n",
        "\n",
        "# Запуск алгоритму навчання з різною кількістю спроб і значеннями gamma\n",
        "different_trials = [50, 100, 200]\n",
        "for trials in different_trials:\n",
        "    learned_policy = learn_mdp_with_unknown_transitions(states, actions, transition_prob, rewards, gamma=0.9, max_iterations=trials)\n",
        "    print(f\"\\nNumber of Trials: {trials}\")\n",
        "    print(\"Learned Policy:\", learned_policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJuLHPpgiqas",
        "outputId": "7e1837d6-c477-4f38-b2af-cbfc8626010b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gamma: 0.5\n",
            "Optimal Policy: [1 1 1 0]\n",
            "Optimal Value Function: [4.18384372 0.33983256 6.85793854 5.05013915]\n",
            "\n",
            "Gamma: 0.9\n",
            "Optimal Policy: [1 1 1 0]\n",
            "Optimal Value Function: [17.97141876 13.2787226  20.33624248 18.75858546]\n",
            "\n",
            "Gamma: 0.99\n",
            "Optimal Policy: [1 1 1 0]\n",
            "Optimal Value Function: [167.3456328  162.43398811 169.63293741 168.11919063]\n",
            "\n",
            "Number of Trials: 50\n",
            "Learned Policy: [0 1 1 0]\n",
            "\n",
            "Number of Trials: 100\n",
            "Learned Policy: [0 0 1 0]\n",
            "\n",
            "Number of Trials: 200\n",
            "Learned Policy: [0 0 0 1]\n"
          ]
        }
      ]
    }
  ]
}